# Explicaci√≥n de Funciones - Codificaci√≥n y Teor√≠a de la Informaci√≥n

## üìö Introducci√≥n

Este c√≥digo implementa conceptos fundamentales de **Teor√≠a de la Informaci√≥n** desarrollados por Claude Shannon, abarcando:

- **Compresi√≥n de Datos**: Algoritmos √≥ptimos de codificaci√≥n (Huffman, Shannon-Fano)
- **Teor√≠a de C√≥digos**: An√°lisis de entrop√≠a, eficiencia y redundancia
- **Detecci√≥n y Correcci√≥n de Errores**: T√©cnicas de control de integridad en transmisiones
- **Verificaci√≥n de Teoremas**: Validaci√≥n del Primer Teorema de Shannon

La complejidad de este c√≥digo radica en la implementaci√≥n rigurosa de conceptos matem√°ticos que subyacen en tecnolog√≠as modernas como compresi√≥n de datos, comunicaciones confiables y almacenamiento eficiente.

---

## üîß FUNCIONES B√ÅSICAS

### 1. `info(val, base)`

**Definici√≥n Matem√°tica:**

$$I(x) = -\log_b(p(x))$$

donde $p(x)$ es la probabilidad de un s√≠mbolo y $b$ es la base logar√≠tmica (base del alfabeto c√≥digo).

**Descripci√≥n:**
Calcula la **autoinformaci√≥n** o **contenido de informaci√≥n** de un s√≠mbolo individual. Esta es una medida fundamental en teor√≠a de la informaci√≥n que cuantifica cu√°nta informaci√≥n (en bits) proporciona la ocurrencia de un evento con probabilidad $p$.

**Interpretaci√≥n:**
- Un evento con alta probabilidad (cercano a 1) tiene baja autoinformaci√≥n
- Un evento con baja probabilidad (cercano a 0) tiene alta autoinformaci√≥n
- Un evento seguro (probabilidad 1) tiene autoinformaci√≥n 0

**Ejemplos:**
```python
info(0.5, 2) = -log‚ÇÇ(0.5) = 1 bit      # Evento con 50% de probabilidad
info(0.25, 2) = -log‚ÇÇ(0.25) = 2 bits   # Evento con 25% de probabilidad
info(1.0, 2) = -log‚ÇÇ(1.0) = 0 bits     # Evento seguro (no aporta informaci√≥n)
```

**Caso especial:** Si `val == 0`, la funci√≥n devuelve 0 (ya que el l√≠mite de $-p\log(p)$ cuando $p \to 0$ es 0).

---

### 2. `generarListaInfo(lista, base)`

**Descripci√≥n:**
Aplica la funci√≥n de autoinformaci√≥n a todos los elementos de una lista mediante una comprensi√≥n de lista (list comprehension).

**F√≥rmula:**
$$I = [I(p_i) : \forall p_i \in \text{lista}]$$

**Funcionalidad:**
Genera el vector de autoinformaci√≥n para un conjunto de probabilidades, permitiendo an√°lisis estad√≠sticos posteriores de la fuente.

```python
generarListaInfo([0.5, 0.25, 0.125], 2)
# Resultado: [1.0, 2.0, 3.0]  (autoinformaci√≥n de cada s√≠mbolo)
```

---

### 3. `generar_lista_longitudes(palabras)`

**Descripci√≥n:**
Obtiene la longitud (n√∫mero de caracteres o s√≠mbolos) de cada palabra c√≥digo en una secuencia.

**F√≥rmula:**
$$L = [|w_i| : \forall w_i \in \text{palabras}]$$

donde $|w_i|$ es la cardinalidad (longitud) de la palabra c√≥digo $w_i$.

**Prop√≥sito:**
Permite calcular posteriormente la longitud media del c√≥digo, un par√°metro cr√≠tico para evaluar la eficiencia de esquemas de codificaci√≥n.

```python
generar_lista_longitudes(["0", "10", "110"])
# Resultado: [1, 2, 3]  (longitudes de cada c√≥digo)
```

---

### 4. `calcular_longitud_media_codigo(palabras_codigo, probabilidades)`

**Definici√≥n Matem√°tica:**

$$L = \sum_{i=0}^{n-1} p_i \cdot |c_i|$$

donde:
- $p_i$ es la probabilidad del s√≠mbolo $i$
- $|c_i|$ es la longitud del c√≥digo asignado al s√≠mbolo $i$
- $n$ es la cardinalidad del alfabeto fuente

**Descripci√≥n:**
Calcula la **longitud media de c√≥digo** (average code length), que representa el n√∫mero esperado de s√≠mbolos binarios requeridos por cada s√≠mbolo de la fuente.

**Significado:**
Es una m√©trica fundamental para evaluar la eficiencia de un c√≥digo. El Primer Teorema de Shannon establece que:

$$H(S) \leq L < H(S) + 1$$

donde $H(S)$ es la entrop√≠a de la fuente.

**Ejemplo:**
```python
# C√≥digos: ["0", "10", "110"]
# Probabilidades: [0.5, 0.3, 0.2]
L = 0.5√ó1 + 0.3√ó2 + 0.2√ó3 = 1.7 bits por s√≠mbolo
```

**Interpretaci√≥n:** En promedio, cada s√≠mbolo de la fuente requiere 1.7 bits para ser codificado.

---

### 5. `obtener_cadena_alfabeto_codigo(palabras)`

**Descripci√≥n:**
Extrae el conjunto de s√≠mbolos distintos (alfabeto c√≥digo) utilizados en una lista de palabras c√≥digo.

**F√≥rmula:**
$$\Sigma = \{c : c \in w_i, \forall w_i \in \text{palabras}\}$$

**Funcionalidad:**
Determina la cardinalidad del alfabeto c√≥digo (n√∫mero de s√≠mbolos distintos), par√°metro esencial para:
- Calcular la base del logaritmo en la entrop√≠a
- Verificar que sea un alfabeto binario (base 2) u otro

```python
obtener_cadena_alfabeto_codigo(["0011", "101", "10"])
# Resultado: "01"  (alfabeto binario)

obtener_cadena_alfabeto_codigo(["ABC", "BAC", "CA"])
# Resultado: "ABC"  (alfabeto ternario)
```

---

### 6. `calcular_entropia_fuente_codigo(palabras_codigo, probabilidades)`

**Definici√≥n Matem√°tica (Entrop√≠a de Shannon):**

$$H_r(S) = -\sum_{i=0}^{n-1} p_i \log_r(p_i)$$

donde:
- $r = |\Sigma|$ es la cardinalidad del alfabeto c√≥digo
- $p_i$ es la probabilidad del s√≠mbolo $i$
- $\log_r$ es el logaritmo en base $r$

**Descripci√≥n:**
Calcula la **entrop√≠a de la fuente en base $r$**, que representa la cantidad m√≠nima de informaci√≥n (medida en s√≠mbolos de base $r$) necesaria en promedio para codificar un s√≠mbolo de la fuente.

**Propiedades Fundamentales:**
1. Es una medida de la incertidumbre o aleatoriedad de la fuente
2. Establece un **l√≠mite inferior te√≥rico** para la longitud media de cualquier c√≥digo
3. La entrop√≠a es m√°xima cuando todos los s√≠mbolos tienen equiprobabilidad

**Teorema de Shannon (Primer Teorema):**

$$H_r(S) \leq L < H_r(S) + 1$$

Esto implica que no existe c√≥digo cuya longitud media sea menor que la entrop√≠a.

**Ejemplo:**
```python
# Probabilidades: [0.5, 0.3, 0.2]
# Alfabeto binario (r=2)
H = -[0.5√ólog‚ÇÇ(0.5) + 0.3√ólog‚ÇÇ(0.3) + 0.2√ólog‚ÇÇ(0.2)]
  = -[0.5√ó(-1) + 0.3√ó(-1.737) + 0.2√ó(-2.322)]
  ‚âà 1.486 bits
```

**Interpretaci√≥n:** Un c√≥digo √≥ptimo para esta fuente debe tener longitud media m√≠nimo de 1.486 bits por s√≠mbolo.

---

### 7. `generarListaExtension(alfabeto, probs, grado)`

**Concepto de Extensi√≥n de Fuente:**

Una extensi√≥n de grado $n$ de una fuente $S$ es una nueva fuente $S^n$ cuyos s√≠mbolos son $n$-tuplas de s√≠mbolos de $S$.

**Descripci√≥n:**
Genera todas las $|\Sigma|^n$ posibles secuencias de longitud $n$ tomadas del alfabeto fuente, junto con sus probabilidades (asumiendo independencia estad√≠stica).

**Algoritmo:**
1. Crea $|\Sigma|^n$ posiciones para almacenar secuencias
2. Para cada posici√≥n, construye la secuencia correspondiente usando aritm√©tica modular
3. Calcula la probabilidad conjunta $P(w_i) = \prod_{j} p_{ij}$ para cada secuencia

**Ejemplo de Extensi√≥n de Grado 2:**
```python
generarListaExtension(["A", "B"], [0.6, 0.4], 2)
# Resultado:
# Secuencias: ["AA", "AB", "BA", "BB"]
# Probabilidades: [0.36, 0.24, 0.24, 0.16]
# (porque 0.6¬≤=0.36, 0.6√ó0.4=0.24, etc.)
```

**Propiedades de la Extensi√≥n:**
- La suma de probabilidades sigue siendo 1
- La entrop√≠a de $S^n$ es $n \times H(S)$
- Cumple el teorema: $H(S^n) = n \times H(S)$

**Aplicaci√≥n:**
Es esencial para verificar el Primer Teorema de Shannon, ya que permite evaluar si un c√≥digo cumple la desigualdad de Shannon en diferentes grados de extensi√≥n.

---

### 8. `cumple_primer_teorema_shannon(probs_fuente, palabras_codigo, n)`

**Primer Teorema de Shannon (Teorema Fundamental de la Codificaci√≥n):**

Para verificar el teorema de Shannon se parte de las probabilidades de la fuente original y las palabras del c√≥digo (debe ser un c√≥digo para la fuente original). Se genera la extensi√≥n de grado $n$ de la fuente y se calcula la entrop√≠a $H_r$ y la longitud media del c√≥digo para la extensi√≥n. Luego se verifica que se cumpla la desigualdad:

$$H_r(S) \leq \frac{L_n}{n} < H_r(S) + \frac{1}{n}$$

donde:
- $H_r(S)$ es la entrop√≠a de la fuente en base $r$ ($r$ es la cantidad de s√≠mbolos distintos en el alfabeto del c√≥digo)
- $L_n$ es la longitud media del c√≥digo para la extensi√≥n de grado $n$
- La divisi√≥n por $n$ normaliza la longitud media al s√≠mbolo original

**Descripci√≥n:**
Implementa la verificaci√≥n rigurosa del Primer Teorema de Shannon. Esta es la verificaci√≥n **te√≥rica fundamental** que garantiza la optimalidad de un c√≥digo.

**Algoritmo:**
1. Genera la extensi√≥n de grado $n$ de la fuente
2. Calcula $L_n$ (longitud media normalizada de la extensi√≥n)
3. Calcula $H_r(S)$ (entrop√≠a normalizada por $n$)
4. Verifica la doble desigualdad

**Conclusi√≥n del Teorema:**
Si se cumple la desigualdad para la extensi√≥n de grado $n$, entonces el c√≥digo cumple el Primer Teorema de Shannon, siendo este un **c√≥digo sin ruido** (noiseless code) que satisface las condiciones de optimalidad de Shannon.

**Interpretaci√≥n Pr√°ctica:**
- Si el c√≥digo es **√≥ptimo**, la desigualdad se cumple y $\frac{L_n}{n}$ se aproxima a $H_r(S)$ cuando $n \to \infty$
- Si el c√≥digo **no es √≥ptimo**, $\frac{L_n}{n}$ ser√° significativamente mayor que $H_r(S)$

**Ejemplo de Verificaci√≥n:**
```python
P = [0.5, 0.3, 0.2]
C = ["0", "10", "11"]  # C√≥digo de Huffman (√≥ptimo)

# Para n=1:
# L = 0.5√ó1 + 0.3√ó2 + 0.2√ó2 = 1.5
# H = -[0.5√ólog‚ÇÇ(0.5) + 0.3√ólog‚ÇÇ(0.3) + 0.2√ólog‚ÇÇ(0.2)] ‚âà 1.486
# Verifica: 1.486 ‚â§ 1.5 < 1.486 + 1 ‚úì

cumple_primer_teorema_shannon(P, C, 1)  # Devuelve True
```

---

### 9. `calcularRedundanciaYEficiencia(probs_extension, palabras_codigo)`

**Definiciones Matem√°ticas:**

**Eficiencia del C√≥digo:**
$$\eta = \frac{H(S)}{L}$$

**Redundancia del C√≥digo:**
$$R = 1 - \eta = 1 - \frac{H(S)}{L}$$

donde:
- $H(S)$ es la entrop√≠a de la fuente (l√≠mite inferior te√≥rico)
- $L$ es la longitud media del c√≥digo

**Descripci√≥n:**
Calcula dos m√©tricas fundamentales para evaluar la calidad de un esquema de codificaci√≥n:

1. **Eficiencia** ($0 \leq \eta \leq 1$): Indica qu√© proporci√≥n de los bits transmitidos contienen informaci√≥n √∫til
2. **Redundancia** ($0 \leq R \leq 1$): Indica qu√© proporci√≥n de bits se utiliza para "llenar" sin aportar informaci√≥n

**Interpretaci√≥n:**
- $\eta = 1$ y $R = 0$: C√≥digo **perfecto/√≥ptimo** (nunca alcanzable en la pr√°ctica)
- $\eta = 0.95$: El 95% de los bits son informaci√≥n, 5% es redundancia
- $\eta < 0.90$: El c√≥digo es **ineficiente** y deber√≠a mejorarse

**Ejemplo Comparativo:**
```python
# C√≥digo 1 (Huffman - √ìptimo):
P = [0.5, 0.3, 0.2]
C1 = ["0", "10", "11"]
# L = 1.5, H ‚âà 1.486
# Œ∑ ‚âà 0.9907, R ‚âà 0.0093  (excelente)

# C√≥digo 2 (No √≥ptimo):
C2 = ["00", "01", "10"]
# L = 2.0, H ‚âà 1.486
# Œ∑ ‚âà 0.7430, R ‚âà 0.2570  (deficiente)

calcularRedundanciaYEficiencia(P, C1)  # (0.0093, 0.9907)
calcularRedundanciaYEficiencia(P, C2)  # (0.2570, 0.7430)
```

**Aplicaci√≥n Pr√°ctica:**
Estas m√©tricas son cr√≠ticas en dise√±o de sistemas de comunicaci√≥n, compresi√≥n de datos y almacenamiento, donde la eficiencia se traduce directamente en ahorro de ancho de banda o espacio de almacenamiento.

---

### 10. `generarListaParalelasCadenaCaracteresYProbs(cadena)`

**Descripci√≥n:**
Realiza un **an√°lisis de frecuencias** de una cadena de caracteres, computando la distribuci√≥n emp√≠rica de probabilidades de los s√≠mbolos.

**Algoritmo:**
1. Recorre la cadena car√°cter por car√°cter
2. Mantiene un registro de s√≠mbolos √∫nicos y sus ocurrencias
3. Calcula la probabilidad emp√≠rica como $p_i = \frac{\text{ocurrencias}_i}{|\text{cadena}|}$

**Salida:**
Tupla $(S, P)$ donde:
- $S$ es la lista de s√≠mbolos √∫nicos (alfabeto observado)
- $P$ es la lista de probabilidades emp√≠ricas correspondientes

**Aplicaci√≥n:**
Herramienta esencial para:
- An√°lisis estad√≠stico de textos reales
- Estimar distribuciones de probabilidad de fuentes naturales
- Preparar datos para algoritmos de compresi√≥n basados en probabilidades (Huffman, Shannon-Fano)

**Ejemplo:**
```python
cadena = "AABBBC"
letras, probs = generarListaParalelasCadenaCaracteresYProbs(cadena)
# letras = ["A", "B", "C"]
# probs = [0.3333, 0.5, 0.1667]  (distribuidas proporcionalmente)
```

**Nota sobre Redondeo:**
Las probabilidades se redondean a 10 decimales para evitar errores de precisi√≥n en c√°lculos posteriores.

---

## üóúÔ∏è ALGORITMOS √ìPTIMOS DE COMPRESI√ìN

### 11. `algoritmo_huffman(probs)`

**Algoritmo de Codificaci√≥n de Huffman:**

Inventado por David A. Huffman (1952), es un algoritmo **greedy** que genera c√≥digos binarios **√≥ptimos** para una distribuci√≥n de probabilidades conocida.

**Propiedades Te√≥ricas:**
- Genera un c√≥digo **√≥ptimo instant√°neo** (sin prefijos)
- La longitud media se acerca a la entrop√≠a: $H(S) \leq L < H(S) + 1$
- Es **√≥ptimo en el sentido de Shannon**

**Algoritmo (Construcci√≥n de √Årbol de Huffman):**
1. Crear nodos hoja para cada s√≠mbolo con su probabilidad
2. Mientras haya m√°s de un nodo:
   - Seleccionar los dos nodos con menor probabilidad
   - Crear nodo padre con probabilidad suma
   - Asignar "0" y "1" a las ramas hijo
3. Recorrer el √°rbol desde ra√≠z a hojas para obtener c√≥digos

**Estructura de Datos:**
Usa una lista de pares `[probabilidad, [√≠ndices]]` donde los √≠ndices rastrean qu√© s√≠mbolos originales corresponden a cada nodo.

**Ejemplo:**
```python
probs = [0.5, 0.3, 0.2]
tabla_huffman = algoritmo_huffman(probs)
# Resultado posible: ["0", "10", "11"]
# S√≠mbolo 0 (prob 0.5) ‚Üí "0" (1 bit)
# S√≠mbolo 1 (prob 0.3) ‚Üí "10" (2 bits)
# S√≠mbolo 2 (prob 0.2) ‚Üí "11" (2 bits)
# L = 0.5√ó1 + 0.3√ó2 + 0.2√ó2 = 1.5 bits
```

**Comparaci√≥n con Shannon-Fano:**
- Huffman: Siempre genera c√≥digo √≥ptimo
- Shannon-Fano: A veces genera c√≥digo sub√≥ptimo

**Aplicaciones Reales:**
- Compresi√≥n ZIP/GZIP
- Compresi√≥n JPEG (tabla de Huffman para DC/AC)
- Transmisi√≥n de datos en redes

---

### 12. `algoritmo_shanon_fano(probs)`

**Algoritmo de Codificaci√≥n de Shannon-Fano:**

Propuesto por Claude Shannon y Robert Fano, es un m√©todo alternativo que genera c√≥digos **casi √≥ptimos** mediante un enfoque **divide y conquista** recursivo.

**Algoritmo (Construcci√≥n Recursiva):**
1. Ordenar s√≠mbolos por probabilidad (descendente)
2. Dividir el conjunto en dos subconjuntos cuyas probabilidades se aproximan a la mitad
3. Asignar "0" al primer subconjunto y "1" al segundo
4. Repetir recursivamente en cada subconjunto hasta que queden s√≠mbolos individuales

**Criterio de Divisi√≥n:**
Para cada partici√≥n, busca el √≠ndice que minimice $|P_{\text{izq}} - P_{\text{der}}|$ donde:
- $P_{\text{izq}} = \sum p_i$ para el subconjunto izquierdo
- $P_{\text{der}} = \sum p_i$ para el subconjunto derecho

**Estructura Recursiva:**
```
def shanon_fano_recursivo(items, tabla_shanon_fano):
    if len(items) > 1:
        # Ordenar, particionar, asignar bits, recursar
```

**Ejemplo Comparativo:**
```python
probs = [0.4, 0.3, 0.2, 0.1]

# Shannon-Fano:
# Nivel 1: [0.4, 0.3] vs [0.2, 0.1]
# Nivel 2: [0.4] vs [0.3] y [0.2] vs [0.1]
# C√≥digos: ["0", "10", "110", "111"]
# L = 0.4√ó1 + 0.3√ó2 + 0.2√ó3 + 0.1√ó3 = 1.9 bits

# Huffman:
# C√≥digos: ["0", "10", "110", "111"]  o similar
# L ‚âà 1.8 bits (mejor que Shannon-Fano en este caso)
```

**Comparaci√≥n:**
| Propiedad | Huffman | Shannon-Fano |
|-----------|---------|--------------|
| Optimalidad | **√ìptimo** | Casi √≥ptimo |
| Complejidad | O(n log n) | O(n log n) |
| Simplicidad | M√°s complejo | M√°s intuitivo |
| Aplicaciones | ZIP, JPEG, MP3 | Educativo, hist√≥rico |

**Nota:** Aunque Shannon-Fano es m√°s directo conceptualmente, Huffman es superior y es el est√°ndar en compresi√≥n moderna.

---

## üîê FUNCIONES DE CODIFICACI√ìN/DECODIFICACI√ìN

### 13. `codificar_en_byteArray(mensaje_a_codificar, alfabeto_fuente, lista_cadena_caracteres)`
**¬øQu√© hace?**
Convierte un mensaje de texto en bytes usando un c√≥digo personalizado.

**Explicaci√≥n simple:**
Si le dices:
- Mensaje: "ABC"
- Alfabeto: ["A", "B", "C"]
- C√≥digos: ["00", "11", "10"]

Devuelve los bytes que representan "001110".

**Para qu√© sirve:** Guardar mensajes codificados en archivos.

---

### 14. `decodificar_de_byteArray(cadena_bits_byteArray, alfabeto_fuente, alfabeto_codigo)`
**¬øQu√© hace?**
Lo inverso del anterior: convierte bytes codificados de vuelta al texto original.

**Explicaci√≥n simple:**
Si le das bytes que representan "001110" y el diccionario de c√≥digos, recupera "ABC".

**Para qu√© sirve:** Leer mensajes codificados.

---

## üì¶ COMPRESI√ìN POR RUN-LENGTH ENCODING

### 15. `codificar_usando_RCL(cadena)`
**¬øQu√© hace?**
Implementa **RLE** (Run-Length Encoding), que almacena repeticiones de forma compacta.

**Explicaci√≥n simple:**
"AAABBBCC" se guarda como:
- A, 3 veces
- B, 3 veces
- C, 2 veces

Es muy efectivo para im√°genes con √°reas de color s√≥lido.

```python
codificar_usando_RCL("AAABBBCC")
# Resultado: bytearray(b'A\x03B\x03C\x02')
```

**Para qu√© sirve:** Comprimir im√°genes simples, FAX, etc.

---

### 16. `calcular_comprension(cadena_alfabeto_fuente, byte_array)`
**¬øQu√© hace?**
Calcula cu√°ntas veces se comprimi√≥ el archivo.

**Explicaci√≥n simple:**
Si el original ten√≠a 100 bytes y comprimido tiene 25, la compresi√≥n es 4 veces.

```python
calcular_comprension("AAABBBCC", byte_array)
# Resultado: 2.66 (se redujo a 37% del tama√±o original)
```

---

## üõ°Ô∏è DETECCI√ìN Y CORRECCI√ìN DE ERRORES

### 17. `hamming(lista_palabras_codigo)`

**Distancia de Hamming:**

La distancia de Hamming entre dos palabras c√≥digo de igual longitud es el n√∫mero de posiciones en las cuales los s√≠mbolos correspondientes **difieren**.

$$d_H(c_i, c_j) = |\{k : c_i[k] \neq c_j[k]\}|$$

**Definici√≥n Operativa:**
```python
d_H("000", "111") = 3  (todas las posiciones difieren)
d_H("001", "011") = 1  (solo la primera posici√≥n difiere)
d_H("0000", "0000") = 0  (c√≥digos id√©nticos)
```

**Descripci√≥n de la Funci√≥n:**
Calcula la **distancia de Hamming m√≠nima** de un c√≥digo (m√≠nima entre todos los pares de palabras) y determina:

1. **Distancia M√≠nima** ($d_{\min}$): El menor n√∫mero de diferencias entre cualquier par de c√≥digos
2. **Errores Detectables**: $t_d = d_{\min} - 1$
3. **Errores Corregibles**: $t_c = \lfloor \frac{d_{\min} - 1}{2} \rfloor$

**Teoremas Fundamentales:**
- **Detecci√≥n:** Un c√≥digo puede detectar hasta $t_d$ errores si $d_{\min} \geq t_d + 1$
- **Correcci√≥n:** Un c√≥digo puede corregir hasta $t_c$ errores si $d_{\min} \geq 2t_c + 1$

**Ejemplo:**
```python
C = ["000", "011", "101", "110"]
hamming(C)
# Distancias: (000,011)‚Üí2, (000,101)‚Üí2, (000,110)‚Üí2, ...
# d_min = 2
# Errores detectables = 1
# Errores corregibles = 0

# Un error de 1 bit se DETECTA pero NO se CORRIGE

C2 = ["0000000", "1111111"]
hamming(C2)
# d_min = 7
# Errores detectables = 6
# Errores corregibles = 3

# Hasta 3 errores se CORRIGEN autom√°ticamente
```

**Aplicaci√≥n en Sistemas Reales:**
- **Comunicaciones Espaciales:** NASA usa c√≥digos con $d_{\min}$ muy alto
- **QR Codes:** Contienen redundancia para recuperarse de da√±o
- **Memoria DRAM:** Usa ECC (Error Correcting Code)

---

### 18. `devolver_byte_con_paridad(byte, tipoParidad=0)`

**Bit de Paridad (Parity Bit):**

Un bit adicional anexado a una palabra c√≥digo que indica si la cantidad de 1s es **par** o **impar**.

**Tipos de Paridad:**
1. **Paridad Par** (Even Parity, `tipoParidad=0`): El bit de paridad es 1 si hay un n√∫mero impar de 1s (para hacer el total par)
2. **Paridad Impar** (Odd Parity, `tipoParidad=1`): El bit de paridad es 1 si hay un n√∫mero par de 1s (para hacer el total impar)

**Algoritmo:**
1. Contar los 1s en el byte original (8 bits)
2. Computar el bit de paridad seg√∫n el tipo
3. Desplazar el byte original a la izquierda 1 posici√≥n
4. Insertar el bit de paridad como el LSB (Least Significant Bit)

**Ejemplo:**
```python
byte_original = "A" = 01000001 (ASCII 65)
Conteo de 1s: 2 (n√∫mero par)

# Paridad par (tipoParidad=0):
Bit de paridad = 0 (ya hay n√∫mero par de 1s)
Resultado: 010000010 = 130 en decimal

# Paridad impar (tipoParidad=1):
Bit de paridad = 1 (para hacer el total impar)
Resultado: 010000011 = 131 en decimal
```

**Estructura del Byte con Paridad:**
```
Bit:     8 7 6 5 4 3 2 1 (0=LSB)
Original: 0 1 0 0 0 0 0 1  (= "A")
Resultado: 0 1 0 0 0 0 1 p  (p = bit de paridad)
```

**Limitaciones:**
- Solo detecta errores **simples** (1 bit)
- No puede corregir errores
- No detecta errores en n√∫mero par de bits

**Aplicaci√≥n:**
Usada en transmisiones serie de bajo nivel, sistemas legados, UART (Universal Asynchronous Receiver-Transmitter)

---

### 19. `byte_tiene_errores(byte, tipoParidad=0)`

**Verificaci√≥n de Bit de Paridad:**

Funci√≥n que **verifica** si el bit de paridad de un byte es correcto (o si hay error en la transmisi√≥n).

**Algoritmo:**
1. Extraer el bit de paridad (LSB)
2. Extraer el byte de datos (primeros 8 bits m√°s significativos)
3. Contar los 1s en el byte de datos
4. Calcular el bit de paridad esperado
5. Comparar bit recibido vs. bit esperado

**Detecci√≥n:**
- Si `bit_paridad == bit_esperado`: **Sin error** ‚Üí devuelve `False`
- Si `bit_paridad ‚â† bit_esperado`: **Error detectado** ‚Üí devuelve `True`

**Ejemplo:**
```python
# Transmisi√≥n correcta:
byte_correcto = devolver_byte_con_paridad("A", tipoParidad=0)
byte_tiene_errores(byte_correcto, tipoParidad=0)  # False

# Transmisi√≥n con error (flip de 1 bit):
byte_con_error = byte_correcto ^ 0x0100  # Flip un bit
byte_tiene_errores(byte_con_error, tipoParidad=0)  # True
```

**Limitaciones de Detecci√≥n Simple:**
- Detecta cambios en n√∫mero **impar** de bits
- **No detecta** cambios en n√∫mero **par** de bits (error no detectado)
- Por ejemplo, si se invierten 2 bits en la transmisi√≥n, el error puede no ser detectado

**Nota:** Es por esto que se necesitan esquemas m√°s robustos como paridad bidimensional (vertical + horizontal)

---

### 20. `codificar_con_paridades(cadena, tipoParidad=0)`

**Esquema de Paridad Bidimensional (2D Parity Check):**

Extensi√≥n sofisticada de paridad que detecta **y corrige** errores simples mediante redundancia en dos dimensiones.

**Componentes del Esquema:**
1. **Paridad Vertical**: 1 bit de paridad por cada byte de datos
2. **Paridad Horizontal (Longitudinal)**: 1 bit de paridad por cada columna
3. **Paridad Cruzada**: 1 bit de paridad del total de paridades longitudinales

**Estructura de Datos:**
```
Datos originales (3 bytes):
      Bit:  8 7 6 5 4 3 2 1 
Byte1:      c‚ÇÅ‚Çà c‚ÇÅ‚Çá ... c‚ÇÅ‚ÇÅ | p‚ÇÅ_v  (paridad vertical)
Byte2:      c‚ÇÇ‚Çà c‚ÇÇ‚Çá ... c‚ÇÇ‚ÇÅ | p‚ÇÇ_v
Byte3:      c‚ÇÉ‚Çà c‚ÇÉ‚Çá ... c‚ÇÉ‚ÇÅ | p‚ÇÉ_v
            ---|---|---|---|---|--+
Paridades h: p_h8 p_h7 ... p_h1 | p_c  (paridad cruzada)
            (longitudinal)        
```

**Capacidades Te√≥ricas:**
- Detecta: **Cualquier n√∫mero de errores** (si el patr√≥n es diferente)
- Corrige: **Un error simple** en posici√≥n conocida
- Localiza: Exactamente d√≥nde est√° el error (intersecci√≥n de fila+columna con error)

**Algoritmo de Codificaci√≥n:**
1. Para cada car√°cter:
   - Calcular paridad vertical
   - Crear byte de 9 bits: 8 de datos + 1 de paridad vertical
2. Para cada columna de la matriz:
   - Calcular paridad horizontal de los 9 bits
3. Calcular paridad cruzada (XOR de todas las paridades horizontales)
4. Retornar: datos con paridades + byte de paridades horizontales + byte de paridad cruzada

**Ejemplo:**
```python
mensaje = "ABC"
resultado = codificar_con_paridades(mensaje, tipoParidad=0)
# Estructura interna (simplificada):
# [A_con_pv, B_con_pv, C_con_pv, paridades_longitudinales, paridad_cruzada]
# Ejemplo: bytearray(b'...')  # Tama√±o = 3 + 2 bytes = 5 bytes

# Con bytes simples ser√≠a: 3 bytes
# Con paridad: 5 bytes (67% overhead pero detecta y corrige errores)
```

**Ventajas sobre Paridad Simple:**
- Localiza el error exactamente
- Puede corregir 1 error autom√°ticamente
- Detecta patrones de errores m√∫ltiples

**Aplicaciones:**
- Transmisi√≥n de datos de larga distancia
- Almacenamiento en cintas magn√©ticas de datos sensibles
- Sistemas aeroespaciales y cr√≠ticos para seguridad
- RAID (Redundant Array of Independent Disks) - an√°logo bidimensional

---

### 21. `decodificar_con_paridades(byte_seq, tipoParidad=0)`
**¬øQu√© hace?**
Lo inverso: verifica todas las paridades y recupera el mensaje original si no hay errores.

---

## üéØ FUNCI√ìN AUXILIAR

### 22. `mostrarListaConIndices(lista)`
**¬øQu√© hace?**
Solo imprime una lista con su √≠ndice de forma bonita.

```python
mostrarListaConIndices(["a", "b", "c"])
# Imprime: [0]=a  [1]=b  [2]=c
```

---

---

# üìù CONCLUSI√ìN: ¬øQU√â ES Y PARA QU√â SIRVE TODO ESTO?

## El Prop√≥sito General

Este c√≥digo implementa **Teor√≠a de la Informaci√≥n**, una rama de la matem√°tica que responde preguntas como:
- ¬øCu√°l es la mejor forma de comprimir un mensaje?
- ¬øC√≥mo detectamos si hay errores en una transmisi√≥n?
- ¬øCu√°ntos bits necesitamos m√≠nimo para codificar informaci√≥n?

## Las 3 Pilares Principales

### 1Ô∏è‚É£ **COMPRESI√ìN DE DATOS** 
Usa algoritmos como Huffman y Shannon-Fano para:
- Hacer archivos m√°s peque√±os
- Usados en: ZIP, JPEG, MP3, videos

**Ejemplo real:** Una pel√≠cula de 2GB se comprime a 500MB

---

### 2Ô∏è‚É£ **DETECCI√ìN Y CORRECCI√ìN DE ERRORES**
Usa t√©cnicas como paridad y distancia de Hamming para:
- Detectar si datos se corrumpieron
- Corregir algunos errores autom√°ticamente
- Usados en: comunicaciones espaciales, QR codes, transmisiones 4G/5G

**Ejemplo real:** El Rover de Marte env√≠a datos a trav√©s del espacio; si se pierden bits, la NASA puede recuperarlos

---

### 3Ô∏è‚É£ **AN√ÅLISIS Y OPTIMIZACI√ìN DE C√ìDIGOS**
Usa entrop√≠a de Shannon y teoremas para:
- Saber si un c√≥digo es √≥ptimo
- Calcular la eficiencia
- Medir redundancia

**Ejemplo real:** Netflix usa estos conceptos para transmitir video sin buffering

---

## Aplicaciones Pr√°cticas

| Aplicaci√≥n | Qu√© Usa |
|-----------|---------|
| **Compresi√≥n ZIP** | Huffman + RLE |
| **Im√°genes JPEG** | Compresi√≥n + Huffman |
| **C√≥digos QR** | Correcci√≥n de errores |
| **Transmisiones 5G** | Paridad + Hamming |
| **Streaming de Video** | Compresi√≥n + an√°lisis de entrop√≠a |
| **Sistemas Satelitales** | Correcci√≥n de errores avanzada |

---

## üéØ S√çNTESIS INTEGRAL

Este c√≥digo es una implementaci√≥n **rigurosa y educativa** de los pilares fundamentales de la Teor√≠a de la Informaci√≥n de Shannon:

### Tres Dominios Principales

**1. COMPRESI√ìN (Entrop√≠a y C√≥digos √ìptimos)**
- Cuantificaci√≥n matem√°tica de informaci√≥n mediante entrop√≠a
- Algoritmos de codificaci√≥n √≥ptima (Huffman, Shannon-Fano)
- Verificaci√≥n del Primer Teorema de Shannon
- Aplicaciones: ZIP, JPEG, MPEG, WebP

**2. DETECCI√ìN Y CORRECCI√ìN DE ERRORES (Robustez)**
- T√©cnicas de control de integridad (paridad, Hamming)
- Localizaci√≥n y correcci√≥n autom√°tica de errores
- Teoremas de detectabilidad y corregibilidad
- Aplicaciones: QR codes, transmisiones satelitales, 5G, RAID

**3. AN√ÅLISIS Y OPTIMIZACI√ìN (Teor√≠a de C√≥digos)**
- M√©tricas de eficiencia y redundancia
- Distribuciones de probabilidad emp√≠ricas
- Extensiones de fuentes y an√°lisis de grado-n

### Relaciones Matem√°ticas Clave

```
Entrop√≠a H(S) ‚Üê L√≠mite inferior fundamental
                      ‚Üì
              Longitud Media L
                      ‚Üì
Eficiencia Œ∑ = H/L, Redundancia R = 1-Œ∑
                      ‚Üì
            Desigualdad de Shannon
        H(S) ‚â§ L < H(S) + 1
                      ‚Üì
              √ìptimalidad del c√≥digo
```

### Flujo de Aplicaci√≥n T√≠pica

```
1. An√°lisis: generarListaParalelasCadenaCaracteresYProbs()
                        ‚Üì
2. Compresi√≥n: algoritmo_huffman() o algoritmo_shanon_fano()
                        ‚Üì
3. Validaci√≥n: cumple_primer_teorema_shannon()
                        ‚Üì
4. Evaluaci√≥n: calcularRedundanciaYEficiencia()
                        ‚Üì
5. Protecci√≥n: codificar_con_paridades()
                        ‚Üì
6. Transmisi√≥n: codificar_en_byteArray()
```

## üìä Tabla Comparativa de M√©todos

| T√©cnica | Optimalidad | Correcci√≥n | Overhead | Uso |
|---------|------------|-----------|----------|-----|
| Huffman | ‚úÖ √ìptimo | ‚ùå No | Bajo | Compresi√≥n |
| Shannon-Fano | ‚ö†Ô∏è Casi √≥ptimo | ‚ùå No | Bajo | Educativo |
| Paridad Simple | ‚ùå No | ‚ùå No | M√≠nimo | Detecci√≥n |
| Paridad 2D | ‚úÖ √ìptimo (1 error) | ‚úÖ S√≠ (1 error) | 67% | Cr√≠tico |
| Hamming | ‚úÖ √ìptimo | ‚úÖ S√≠ | Moderado | Memoria |

## üåê Contexto Hist√≥rico

- **Claude Shannon** (1948): Funda la Teor√≠a de la Informaci√≥n
- **David Huffman** (1952): Desarrolla algoritmo de codificaci√≥n √≥ptima
- **Richard Hamming** (1950): Introduce c√≥digos detectores/correctores
- **Modern Era**: Aplicaci√≥n universal en todas las comunicaciones digitales

---

**Conclusi√≥n:** Cuando usas internet, streaming de video, compresi√≥n de archivos, comunicaciones 4G/5G, sat√©lites GPS o escaneas un c√≥digo QR, **est√°s usando directamente estos principios y algoritmos de Teor√≠a de la Informaci√≥n**.

Este c√≥digo es una **encarnaci√≥n del conocimiento fundamental** que hace funcionar la era digital moderna.

---

**Escrito para:** Estudiantes de Ingenier√≠a en Sistemas, Telecomunicaciones, Inform√°tica | **Nivel Acad√©mico:** Teor√≠a de la Informaci√≥n (Pregrado Avanzado/Postgrado)
