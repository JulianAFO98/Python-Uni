# Teor√≠a de Canales Estoc√°sticos: An√°lisis Matem√°tico e Implementaci√≥n

## üìö Introducci√≥n Conceptual

Un **canal estoc√°stico** (o canal ruidoso) es un modelo probabil√≠stico que describe la transmisi√≥n de informaci√≥n a trav√©s de un medio que puede introducir perturbaciones, errores o corrupci√≥n de datos.

### Definici√≥n Formal

Un canal discreto sin memoria (DMC - Discrete Memoryless Channel) se define como una tupla $\mathcal{C} = (X, Y, p_{Y|X})$ donde:

- $X = \{x_1, x_2, \ldots, x_m\}$ es el **alfabeto de entrada** (s√≠mbolos transmitidos)
- $Y = \{y_1, y_2, \ldots, y_n\}$ es el **alfabeto de salida** (s√≠mbolos recibidos)
- $p_{Y|X}(y_j | x_i)$ es la **matriz de transici√≥n** que define las probabilidades condicionales de recibir $y_j$ dado que se transmiti√≥ $x_i$

Este c√≥digo implementa herramientas para **an√°lisis riguroso** de canales, c√°lculo de capacidad, informaci√≥n mutua y probabilidades de error.

---

## üîß FUNCIONES DE CONSTRUCCI√ìN DE MATRICES

### 1. `generar_matriz_canal(cadena_sin_codificar, cadena_salida)`

**Descripci√≥n:**
Construye la **matriz de transici√≥n del canal** (matriz de probabilidades condicionales) a partir de dos cadenas emp√≠ricas: una de entrada observada y una de salida observada.

**Proceso Matem√°tico:**

$$P(y_j | x_i) = \frac{\text{N√∫mero de veces que } y_j \text{ se observa cuando se transmiti√≥ } x_i}{\text{N√∫mero total de veces que se transmiti√≥ } x_i}$$

**Algoritmo:**
1. Extraer alfabetos √∫nicos de ambas cadenas (ordenados alfab√©ticamente)
2. Crear matriz de conteos: $C[i][j]$ = n√∫mero de pares $(x_i, y_j)$ observados
3. Normalizar por filas: dividir cada elemento $C[i][j]$ por $\sum_j C[i][j]$ (suma de la fila)
4. Redondear a 3 decimales para precisi√≥n

**Ejemplo:**
```python
entrada = "abcacaabbcacaabcacaaabcaca"
salida = "01010110011001000100010011"

# Resultado: matriz 3√ó2 donde:
# Filas: a, b, c (entrada)
# Columnas: 0, 1 (salida)
# Valores: P(salida | entrada)
```

**Interpretaci√≥n:**
Cada fila representa la distribuci√≥n de probabilidad sobre las salidas posibles condicionada a una entrada espec√≠fica.

---

### 2. `a_priori(cadena)`

**Descripci√≥n:**
Calcula la **distribuci√≥n de probabilidades a priori** (distribuci√≥n marginal de entrada) a partir de una cadena observada.

**F√≥rmula:**
$$P(x_i) = \frac{\text{Ocurrencias de } x_i}{|\text{cadena}|}$$

**Retorno:**
Lista de probabilidades ordenadas alfab√©ticamente, paralela a las filas de la matriz del canal.

**Relaci√≥n con la Matriz del Canal:**
```
Estructura de datos:
P_priori = [P(a), P(b), P(c)]  ‚Üê paralela a filas

Matriz =  [[P(y1|a), P(y2|a)],
           [P(y1|b), P(y2|b)],
           [P(y1|c), P(y2|c)]]
```

---

## üìä MATRICES DERIVADAS DEL CANAL

### 3. `generar_probs_salida(probs_priori, matriz_canal)`

**Definici√≥n Matem√°tica (Probabilidades Marginales de Salida):**

$$P(y_j) = \sum_{i=1}^{m} P(x_i) \cdot P(y_j | x_i)$$

**Descripci√≥n:**
Calcula la **distribuci√≥n de probabilidades de salida** (marginales), aplicando el teorema de la probabilidad total sobre todas las posibles entradas.

**Interpretaci√≥n:**
La probabilidad de observar un s√≠mbolo en la salida del canal es la suma ponderada de todas las formas posibles en que ese s√≠mbolo puede ser generado desde las entradas.

**Ejemplo:**
```python
P_entrada = [0.3, 0.3, 0.4]
P_salida = generar_probs_salida(P_entrada, matriz_3x2)
# Resultado: [P(y1), P(y2)] distribuci√≥n marginal de salida
```

---

### 4. `generar_matriz_eventos_simultaneos(probs_priori, matriz_canal)`

**Definici√≥n Matem√°tica (Probabilidades Conjuntas):**

$$P(x_i, y_j) = P(x_i) \cdot P(y_j | x_i)$$

**Descripci√≥n:**
Genera la **matriz de probabilidades conjuntas** $P(X, Y)$, que representa la probabilidad de que simult√°neamente se transmita $x_i$ y se reciba $y_j$.

**Propiedades:**
- Cada elemento est√° entre 0 y 1
- La suma de todos los elementos es 1 (forma una distribuci√≥n de probabilidad)
- Es la informaci√≥n m√°s completa sobre el comportamiento del canal con distribuci√≥n de entrada conocida

**Relaci√≥n Matem√°tica:**
```
Sumando por filas: ‚àë_j P(x_i, y_j) = P(x_i)
Sumando por columnas: ‚àë_i P(x_i, y_j) = P(y_j)
Sumando todo: ‚àë_i ‚àë_j P(x_i, y_j) = 1
```

---

### 5. `generar_matriz_posteriori(probs_priori, matriz_canal)`

**Definici√≥n Matem√°tica (Teorema de Bayes):**

$$P(x_i | y_j) = \frac{P(x_i, y_j)}{P(y_j)} = \frac{P(x_i) \cdot P(y_j | x_i)}{\sum_{i'} P(x_{i'}) \cdot P(y_j | x_{i'})}$$

**Descripci√≥n:**
Calcula la **matriz de probabilidades posteriori**, que representa la creencia sobre cu√°l entrada se transmiti√≥ **dado que** se observ√≥ una salida espec√≠fica. Este es el c√°lculo fundamental para:
- **Demodulaci√≥n y detecci√≥n** en comunicaciones
- **Correcci√≥n de errores** en canales ruidosos
- **Toma de decisiones √≥ptimas** en receptores

**Interpretaci√≥n:**
Responde la pregunta: "Si observ√© $y_j$ a la salida del canal, ¬øcu√°l es la probabilidad de que se haya transmitido cada $x_i$?"

**Caso Especial - Divisi√≥n por Cero:**
Si $P(y_j) = 0$ (evento imposible), se asigna $P(x_i | y_j) = 0$ para evitar errores num√©ricos.

---

## üìà FUNCIONES DE VISUALIZACI√ìN Y UTILIDAD

### 6. `mostrar_matriz_encuadrada(matriz, etiquetas_filas, etiquetas_columnas)`

**Descripci√≥n:**
Imprime una matriz de forma elegante con:
- Etiquetas para filas y columnas
- Formato num√©rico consistente (4 decimales)
- Separadores visuales (l√≠neas)
- Alineaci√≥n clara

**Uso T√≠pico:**
```python
mostrar_matriz_encuadrada(matriz_posteriori, 
                         etiquetas_filas=['A', 'B', 'C'],
                         etiquetas_columnas=['0', '1', '2'])
```

### 7. `info(num)`

**Definici√≥n:**
$$I(x) = -\log_2(x) \text{ si } x \neq 0, \quad \text{si no } 0$$

Funci√≥n de autoinformaci√≥n (en bits). Misma definici√≥n que en `shanon_huffman.py`.

---

## üßÆ FUNCIONES DE ENTROP√çA E INFORMACI√ìN

### 8. `calcular_entropia_salida(probs_priori, matriz_canal)`

**Definici√≥n Matem√°tica:**

$$H(Y) = -\sum_{j=1}^{n} P(y_j) \log_2 P(y_j)$$

**Descripci√≥n:**
Calcula la **entrop√≠a de la salida del canal**, que mide la incertidumbre en los s√≠mbolos observados a la salida.

**Propiedades:**
- Representa cu√°nta informaci√≥n (en bits) contiene en promedio cada s√≠mbolo de salida
- Depende tanto de la matriz del canal como de la distribuci√≥n de entrada
- M√°ximo valor: $\log_2 n$ (cuando salidas equiprobables)

---

### 9. `lista_entropias(probs_priori, matriz_canal)`

**Retorna:**
Tupla $(H(X), [H(X|y_1), H(X|y_2), \ldots, H(X|y_n)])$

**Definiciones Matem√°ticas:**

**Entrop√≠a A Priori (entrada):**
$$H(X) = -\sum_{i=1}^{m} P(x_i) \log_2 P(x_i)$$

**Entrop√≠a Posteriori Condicional (dado cada s√≠mbolo de salida):**
$$H(X | y_j) = -\sum_{i=1}^{m} P(x_i | y_j) \log_2 P(x_i | y_j)$$

**Descripci√≥n:**
- $H(X)$ es la incertidumbre sobre la entrada **antes** de recibir nada
- $H(X|y_j)$ es la incertidumbre remanente sobre la entrada **despu√©s** de observar la salida $y_j$

**Interpretaci√≥n:**
La reducci√≥n $H(X) - H(X|y_j)$ representa cu√°nta informaci√≥n sobre la entrada proporciona observar $y_j$.

---

### 10. `calcular_equivocacion(probs_priori, matriz_canal)`

**Definiciones Matem√°ticas:**

**Equivocaci√≥n (P√©rdida de Informaci√≥n - Confusi√≥n):**
$$H(X|Y) = \sum_{j=1}^{n} P(y_j) \cdot H(X|y_j)$$

Promedio ponderado de las entrop√≠as condicionales posteriori.

**P√©rdida (Ruido):**
$$H(Y|X) = \sum_{i=1}^{m} P(x_i) \cdot H(Y|x_i)$$

donde $H(Y|x_i) = -\sum_{j} P(y_j|x_i) \log_2 P(y_j|x_i)$

**Descripci√≥n:**
- **$H(X|Y)$ (Equivocaci√≥n):** Incertidumbre remanente sobre la entrada **despu√©s** de observar la salida. Representa el error potencial en la decodificaci√≥n.
- **$H(Y|X)$ (P√©rdida):** Entrop√≠a de la salida condicionada a la entrada. Representa cu√°nta informaci√≥n se pierde en la transmisi√≥n.

**Relaci√≥n Fundamental:**
$$I(X;Y) = H(X) - H(X|Y) = H(Y) - H(Y|X)$$

---

### 11. `calcular_entropia_afin(probs_priori, matriz_canal)`

**Definici√≥n Matem√°tica (Entrop√≠a Conjunta):**

$$H(X,Y) = -\sum_{i,j} P(x_i, y_j) \log_2 P(x_i, y_j)$$

**Descripci√≥n:**
Entrop√≠a del sistema conjunto $(X, Y)$. Mide la incertidumbre total en el par entrada-salida.

**Relaci√≥n con otras Entrop√≠as:**
$$H(X,Y) = H(X) + H(Y|X) = H(Y) + H(X|Y)$$

---

### 12. `calcular_informacion_mutua(probs_priori, matriz_canal)`

**Definici√≥n Matem√°tica (Informaci√≥n Mutua):**

$$I(X;Y) = \sum_{i,j} P(x_i, y_j) \log_2 \left( \frac{P(x_i, y_j)}{P(x_i) P(y_j)} \right)$$

**Descripci√≥n:**
Cuantifica la **cantidad de informaci√≥n compartida** entre entrada y salida del canal. Mide cu√°nta informaci√≥n sobre la entrada se puede obtener observando la salida.

**Propiedades Te√≥ricas:**
- $I(X;Y) \geq 0$ (siempre no negativa)
- $I(X;Y) = 0$ si y solo si $X$ e $Y$ son independientes
- $I(X;Y) = H(X)$ si el canal es sin ruido (decodificaci√≥n perfecta)
- $I(X;Y) = H(X) - H(X|Y)$ (informaci√≥n remanente despu√©s de la transmisi√≥n)

**Interpretaci√≥n:**
Es el par√°metro fundamental para la **Capacidad del Canal** (Shannon).

---

## üîç CLASIFICACI√ìN DE CANALES

### 13. `es_sin_ruido(matriz_canal)`

**Definici√≥n:**
Un canal es **sin ruido** (noiseless) si cada columna contiene exactamente un elemento no nulo.

**Interpretaci√≥n:**
Para cada salida $y_j$, existe exactamente una entrada $x_i$ que puede producirla. No hay ambig√ºedad: conocer la salida determina un√≠vocamente la entrada.

**Propiedad Matem√°tica:**
$$I(X;Y) = H(X)$$

(Toda la informaci√≥n de entrada se preserva en la salida)

---

### 14. `es_determinante(matriz_canal)`

**Definici√≥n:**
Un canal es **determinante** (deterministic) si cada fila contiene exactamente un elemento no nulo.

**Interpretaci√≥n:**
Para cada entrada $x_i$, la salida $y_j$ est√° determinada de forma un√≠voca (no hay aleatoriedad en la transmisi√≥n desde el lado de entrada).

**Propiedad Matem√°tica:**
$$H(Y|X) = 0$$

(No hay p√©rdida de informaci√≥n; el ruido viene de la ambig√ºedad en la decodificaci√≥n)

---

### 15. `es_canal_uniforme(matriz_canal)`

**Definici√≥n:**
Un canal es **uniforme** si cada fila de la matriz de transici√≥n es una permutaci√≥n de la primera fila.

**Interpretaci√≥n:**
El comportamiento probabil√≠stico es "sim√©trico" en todas las entradas: cambiar de entrada mantiene la distribuci√≥n sobre salidas (solo cambia el orden).

**Propiedad Importante:**
$$H(Y|X) = \text{constante}$$

(P√©rdida de informaci√≥n independiente de la entrada transmitida)

**Capacidad de Canal Uniforme:**
$$C = \log_2 m - H(Y|X)$$

---

## üîó OPERACIONES CON CANALES

### 16. `generar_matriz_compuesta(A, B)`

**Descripci√≥n:**
Multiplica dos matrices de transici√≥n de canales para obtener el canal **compuesto** o en **cascada**.

**Interpretaci√≥n F√≠sica:**
Si $A$ representa un canal discreto y $B$ representa otro canal discreto, el producto $A \times B$ representa pasar la salida de $A$ como entrada a $B$.

**Matrices Requeridas:**
- Dimensiones: $A$ es $m \times n$, $B$ es $n \times p$
- Resultado: matriz $m \times p$ (canal compuesto)

**F√≥rmula:**
$$P(z_k|x_i) = \sum_{j=1}^{n} P(y_j|x_i) \cdot P(z_k|y_j)$$

---

### 17. `_verificar_proporcionalidad(matriz, col_a, col_b, tol)`

**Descripci√≥n:**
Verifica si dos columnas de una matriz son **proporcionales** (dentro de tolerancia num√©rica).

**Matem√°ticamente:**
Dos columnas son proporcionales si $\exists k : col_a[i] = k \cdot col_b[i] \; \forall i$

**Uso Interno:**
Paso intermedio para simplificar canales mediante reducci√≥n de columnas combinables.

---

### 18. `son_columnas_combinables(matriz, col1, col2)`

**Descripci√≥n:**
Determina si dos columnas pueden ser **combinadas** (fusionadas) sin perder informaci√≥n.

**Criterio:**
Dos columnas son combinables si son proporcionales en **cualquier direcci√≥n** (considerando ambas direcciones de proporcionalidad).

**Significado Te√≥rico:**
Las salidas correspondientes a esas columnas son estad√≠sticamente indistinguibles desde el receptor; combinarlas simplifica el canal sin cambiar sus propiedades esenciales.

---

### 19. `generar_matriz_determinante(matriz, col1, col2)`

**Descripci√≥n:**
Crea una **matriz de determinante** $D$ para combinar dos columnas especificadas en una sola.

**Operaci√≥n:**
Genera una matriz que, multiplicada con la matriz del canal, produce una versi√≥n reducida combinando las dos columnas.

---

### 20. `generar_matriz_reducida(matriz_de_un_canal)`

**Descripci√≥n:**
Realiza **todas las reducciones posibles** de un canal combinando columnas proporcionales de forma iterativa.

**Algoritmo:**
```
MIENTRAS haya reducciones posibles:
    POR CADA par de columnas:
        SI son combinables:
            Crear matriz D
            Multiplicar canal √ó D
            Repetir desde inicio
```

**Resultado:**
Canal can√≥nico o **forma reducida** equivalente, con n√∫mero m√≠nimo de salidas distinguibles.

---

## ‚ö° CAPACIDAD DEL CANAL

### 21. `calcular_capacidad_canal(matriz_canal)`

**Definici√≥n Te√≥rica (Capacidad de Shannon):**

$$C = \max_{P(X)} I(X;Y) \text{ (bits por s√≠mbolo)}$$

**F√≥rmulas por Tipo de Canal:**

**1. Canal Determinante** (deterministic):
$$C = \log_2 n \quad (n = \text{n√∫mero de salidas})$$

**2. Canal Sin Ruido** (noiseless):
$$C = \log_2 m \quad (m = \text{n√∫mero de entradas})$$

**3. Canal Uniforme:**
$$C = \log_2 m - H(Y|X)$$

donde $H(Y|X)$ es la entrop√≠a condicional (constante para canales uniformes).

**Descripci√≥n:**
La capacidad es la **tasa m√°xima de informaci√≥n** (en bits) que se puede transmitir por s√≠mbolo del canal de forma confiable.

**Teorema de Shannon:**
Para cualquier $R < C$ (tasa menor que capacidad), existe un c√≥digo que permite transmitir con error arbitrariamente peque√±o.

---

### 22. `estimar_capacidad_canal_binario(matriz_canal, paso)`

**Descripci√≥n:**
Estima la capacidad de un **canal binario** probando distribuciones de entrada uniformes seg√∫n un paso especificado.

**Algoritmo:**
1. Para probabilidades a priori $p = 0, \text{paso}, 2 \cdot \text{paso}, \ldots, 1$:
   - Calcular informaci√≥n mutua $I(X;Y)$ para $P(x_1) = p, P(x_2) = 1-p$
2. Retornar el m√°ximo de informaci√≥n mutua y su probabilidad asociada

**Retorna:**
- Capacidad estimada (m√°ximo de $I(X;Y)$)
- Probabilidad a priori que produce ese m√°ximo

**Precisi√≥n:**
Inversamente proporcional al `paso`: paso m√°s peque√±o ‚Üí estimaci√≥n m√°s precisa (pero m√°s c√°lculos).

**Ejemplo:**
```python
matriz_binaria = [[0.6, 0.4], [0.2, 0.8]]
capacidad, probs = estimar_capacidad_canal_binario(matriz_binaria, 0.001)
# paso=0.001 realiza 1001 iteraciones
```

---

## üéØ PROBABILIDAD DE ERROR Y DECISI√ìN

### 23. `calcular_probabilidad_error(probs_priori, matriz_canal)`

**Descripci√≥n:**
Calcula la **probabilidad de error m√≠nima** usando la **regla de decisi√≥n ML (Maximum Likelihood)**.

**Algoritmo de Decisi√≥n:**

1. **Regla de decisi√≥n:** Para cada salida $y_j$, decidir que se transmiti√≥:
   $$\hat{x}_j = \arg\max_i P(x_i | y_j)$$

2. **Probabilidad de acierto dado $x_i$ transmitido:**
   $$P(\text{acierto}|x_i) = \sum_{j: \hat{x}_j = x_i} P(y_j|x_i)$$

3. **Probabilidad de error dado $x_i$ transmitido:**
   $$P(\text{error}|x_i) = 1 - P(\text{acierto}|x_i)$$

4. **Probabilidad de error total:**
   $$P_e = \sum_{i=1}^{m} P(x_i) \cdot P(\text{error}|x_i)$$

**Interpretaci√≥n:**
Es el error m√≠nimo alcanzable con un decodificador √≥ptimo (ML).

**Ejemplo:**
```python
matriz = [[0.6, 0.3, 0.1], 
          [0.1, 0.8, 0.1],
          [0.3, 0.3, 0.4]]
probs = [4/15, 3/15, 8/15]
Pe = calcular_probabilidad_error(probs, matriz)
# Resultado: probabilidad de error con decisi√≥n √≥ptima
```

---

## üéì S√çNTESIS INTEGRAL: TEOR√çA DE CANALES

### Conceptos Fundamentales

**1. Transmisi√≥n de Informaci√≥n:**
$$\text{Fuente} \xrightarrow{P(X)} \text{Canal} \xrightarrow{P(Y|X)} \text{Receptor}$$

**2. Medidas Clave:**
| Concepto | S√≠mbolo | Significado |
|----------|---------|-----------|
| Entrop√≠a entrada | $H(X)$ | Incertidumbre sobre entrada |
| Entrop√≠a salida | $H(Y)$ | Incertidumbre sobre salida |
| Entrop√≠a conjunta | $H(X,Y)$ | Incertidumbre total |
| Equivocaci√≥n | $H(X\|Y)$ | Incertidumbre remanente (error potencial) |
| P√©rdida | $H(Y\|X)$ | Informaci√≥n perdida en el canal |
| Informaci√≥n mutua | $I(X;Y)$ | Informaci√≥n compartida |

**3. Relaciones Te√≥ricas:**
$$H(X,Y) = H(X) + H(Y|X)$$
$$I(X;Y) = H(X) + H(Y) - H(X,Y)$$
$$I(X;Y) = H(X) - H(X|Y) = H(Y) - H(Y|X)$$

**4. Capacidad:**
$$C = \max_P I(X;Y)$$

Tasa m√°xima de transmisi√≥n confiable (en bits/s√≠mbolo).

### Clasificaci√≥n de Canales

| Tipo | Propiedad | $I(X;Y)$ | $C$ |
|------|----------|---------|-----|
| **Sin Ruido** | Cada salida de 1 entrada | $H(X)$ | $\log_2 m$ |
| **Determinante** | Cada entrada ‚Üí 1 salida | $H(Y)$ | $\log_2 n$ |
| **Uniforme** | Filas son permutaciones | Funci√≥n de $P(X)$ | $\log_2 m - H_0$ |
| **General** | Ruido en ambas direcciones | $\leq \min(H(X), H(Y))$ | $\leq \min(H(X), H(Y))$ |

### Aplicaciones Pr√°cticas

1. **Comunicaciones Digitales:** Dise√±o de moduladores/demoduladores
2. **Almacenamiento:** C√≥digos correctores de errores
3. **Redes:** Asignaci√≥n √≥ptima de potencia y recursos
4. **Criptograf√≠a:** An√°lisis de seguridad de canales
5. **Compresi√≥n:** Codificaci√≥n de fuentes para canales ruidosos

---

## üìà Ejemplo Completo de An√°lisis

```python
# Dados
entrada = "1101011001..."
salida = "1001111111..."

# Paso 1: Construir matriz del canal
matriz = generar_matriz_canal(entrada, salida)

# Paso 2: Obtener distribuci√≥n de entrada
P_entrada = a_priori(entrada)

# Paso 3: Calcular derivadas
P_salida = generar_probs_salida(P_entrada, matriz)
P_simul = generar_matriz_eventos_simultaneos(P_entrada, matriz)
P_post = generar_matriz_posteriori(P_entrada, matriz)

# Paso 4: Analizar entrop√≠as
H_entrada, H_post_lista = lista_entropias(P_entrada, matriz)
H_eq, H_perd = calcular_equivocacion(P_entrada, matriz)

# Paso 5: Medir informaci√≥n
I_mutua = calcular_informacion_mutua(P_entrada, matriz)

# Paso 6: Clasificar canal
print(f"¬øSin ruido? {es_sin_ruido(matriz)}")
print(f"¬øDeterminante? {es_determinante(matriz)}")
print(f"¬øUniforme? {es_canal_uniforme(matriz)}")

# Paso 7: Capacidad y decisi√≥n
C = calcular_capacidad_canal(matriz)
P_error = calcular_probabilidad_error(P_entrada, matriz)
```

---

**Conclusi√≥n:** El an√°lisis de canales estoc√°sticos es la base te√≥rica para toda comunicaci√≥n digital confiable. Estos conceptos permiten dise√±ar sistemas √≥ptimos que alcanzan los l√≠mites fundamentales establecidos por Shannon.

